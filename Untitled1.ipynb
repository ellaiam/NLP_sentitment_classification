{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To train and load text classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "text_information = pd.read_csv('/home/praveen/Downloads/jigsaw-toxic-comment-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_information = pd.DataFrame(data=text_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_counts = text_information[text_information['toxic'] == 1].index.tolist()\n",
    "severe_toxic_counts = text_information[text_information['severe_toxic'] == 1].index.tolist()\n",
    "obscene_counts = text_information[text_information['obscene'] == 1].index.tolist()\n",
    "threat_counts = text_information[text_information['threat'] == 1].index.tolist()\n",
    "insult_counts = text_information[text_information['insult'] == 1].index.tolist()\n",
    "identity_hate_counts = text_information[text_information['identity_hate'] == 1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15294 1595 8449 478 7877 1405\n"
     ]
    }
   ],
   "source": [
    "print(len(toxic_counts),len(severe_toxic_counts),len(obscene_counts),\n",
    "     len(threat_counts),len(insult_counts),len(identity_hate_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_comments = []\n",
    "for values in toxic_counts:\n",
    "    important_comments.append(text_information.iloc[values])\n",
    "    \n",
    "for values1 in severe_toxic_counts:\n",
    "    important_comments.append(text_information.iloc[values1])\n",
    "    \n",
    "for values2 in obscene_counts:\n",
    "    important_comments.append(text_information.iloc[values2])\n",
    "    \n",
    "for values3 in threat_counts:\n",
    "    important_comments.append(text_information.iloc[values3])\n",
    "    \n",
    "for values4 in insult_counts:\n",
    "    important_comments.append(text_information.iloc[values4])\n",
    "    \n",
    "for values5 in identity_hate_counts:\n",
    "    important_comments.append(text_information.iloc[values5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_comments = pd.DataFrame(data=important_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_comments = important_comments.drop(columns = ['id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data = important_comments[(important_comments['toxic'] == 1) &\n",
    "                                       (important_comments['severe_toxic'] == 0) &\n",
    "                                       (important_comments['obscene'] == 0) & \n",
    "                                       (important_comments['threat'] == 0) & \n",
    "                                       (important_comments['insult'] == 0) & \n",
    "                                       (important_comments['identity_hate'] == 0)].index.tolist()\n",
    "\n",
    "severe_toxic_data = important_comments[(important_comments['toxic'] == 0) &\n",
    "                                       (important_comments['severe_toxic'] == 1) &\n",
    "                                       (important_comments['obscene'] == 0) & \n",
    "                                       (important_comments['threat'] == 0) & \n",
    "                                       (important_comments['insult'] == 0) & \n",
    "                                       (important_comments['identity_hate'] == 0)].index.tolist()\n",
    "\n",
    "obscene_data = important_comments[(important_comments['toxic'] == 0) &\n",
    "                                       (important_comments['severe_toxic'] == 0) &\n",
    "                                       (important_comments['obscene'] == 1) & \n",
    "                                       (important_comments['threat'] == 0) & \n",
    "                                       (important_comments['insult'] == 0) & \n",
    "                                       (important_comments['identity_hate'] == 0)].index.tolist()\n",
    "\n",
    "threat_data = important_comments[(important_comments['toxic'] == 0) &\n",
    "                                       (important_comments['severe_toxic'] == 0) &\n",
    "                                       (important_comments['obscene'] == 0) & \n",
    "                                       (important_comments['threat'] == 1) & \n",
    "                                       (important_comments['insult'] == 0) & \n",
    "                                       (important_comments['identity_hate'] == 0)].index.tolist()\n",
    "\n",
    "insult_data = important_comments[(important_comments['toxic'] == 0) &\n",
    "                                       (important_comments['severe_toxic'] == 0) &\n",
    "                                       (important_comments['obscene'] == 0) & \n",
    "                                       (important_comments['threat'] == 0) & \n",
    "                                       (important_comments['insult'] == 1) & \n",
    "                                       (important_comments['identity_hate'] == 0)].index.tolist()\n",
    "\n",
    "identity_data = important_comments[(important_comments['toxic'] == 0) &\n",
    "                                       (important_comments['severe_toxic'] == 0) &\n",
    "                                       (important_comments['obscene'] == 0) & \n",
    "                                       (important_comments['threat'] == 0) & \n",
    "                                       (important_comments['insult'] == 0) & \n",
    "                                       (important_comments['identity_hate'] == 1)].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_description = []\n",
    "obscene_description = []\n",
    "threat_description = []\n",
    "insult_description = []\n",
    "identity_description = []\n",
    "\n",
    "for i in toxic_data:\n",
    "    toxic_description.append(important_comments.loc[i])\n",
    "    \n",
    "for k in obscene_data:\n",
    "    obscene_description.append(important_comments.loc[k])\n",
    "    \n",
    "for l in threat_data:\n",
    "    threat_description.append(important_comments.loc[l])\n",
    "    \n",
    "for m in insult_data:\n",
    "    insult_description.append(important_comments.loc[m])\n",
    "    \n",
    "for n in identity_data:\n",
    "    identity_description.append(important_comments.loc[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_description = pd.DataFrame(data=toxic_description)\n",
    "obscene_description = pd.DataFrame(data=obscene_description)\n",
    "threat_description = pd.DataFrame(data=threat_description)\n",
    "insult_description = pd.DataFrame(data=insult_description)\n",
    "identity_description = pd.DataFrame(data=identity_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "def punctuation_lowercase(document):\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    stripped = [lines.translate(table) for lines in document['comment_text']]\n",
    "    words = [word.lower() for word in stripped]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments = punctuation_lowercase(toxic_description)\n",
    "obscene_comments = punctuation_lowercase(obscene_description)\n",
    "threat_comments = punctuation_lowercase(threat_description)\n",
    "insult_comments = punctuation_lowercase(insult_description)\n",
    "identity_comments = punctuation_lowercase(identity_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_description = toxic_description.drop(columns = ['comment_text'])\n",
    "obscene_description = obscene_description.drop(columns = ['comment_text'])\n",
    "threat_description = threat_description.drop(columns=['comment_text'])\n",
    "insult_description = insult_description.drop(columns=['comment_text'])\n",
    "identity_description = identity_description.drop(columns=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_description['comment_text'] = toxic_comments\n",
    "obscene_description['comment_text'] = obscene_comments\n",
    "threat_description['comment_text'] = threat_comments\n",
    "insult_description['comment_text'] = insult_comments\n",
    "identity_description['comment_text'] = identity_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "document_name = identity_description\n",
    "\n",
    "toxic_file = Path('identity.txt')\n",
    "file_object = open(toxic_file,'w')\n",
    "\n",
    "for i in range(len(document_name['comment_text'])):\n",
    "    contents = document_name['comment_text'].iloc[i].replace(\"\\n\", \" \")\n",
    "    toxic = 'identity'\n",
    "    output_lines = '__label__.{} {}'.format(toxic,contents)\n",
    "                                               \n",
    "    file_object.write(output_lines + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ['/home/praveen/tensorflow_python/lib/python3.6/site-packages/tensorflow/models/research/toxic.txt',\n",
    "           '/home/praveen/tensorflow_python/lib/python3.6/site-packages/tensorflow/models/research/obscene.txt',\n",
    "           '/home/praveen/tensorflow_python/lib/python3.6/site-packages/tensorflow/models/research/threat.txt',\n",
    "           '/home/praveen/tensorflow_python/lib/python3.6/site-packages/tensorflow/models/research/insult.txt',\n",
    "           '/home/praveen/tensorflow_python/lib/python3.6/site-packages/tensorflow/models/research/identity.txt']\n",
    "\n",
    "with open('file','w') as outfile:\n",
    "    for fname in filename:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('file','r') as entire_file, open('final_file','w') as final_file:\n",
    "    for lines in entire_file:\n",
    "        texts = re.sub('[0-9]+','',lines)\n",
    "        final_file.write(texts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised(input='/home/praveen/tensorflow_python/lib/python3.6/site-packages/tensorflow/models/research/final_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__.toxic',), array([0.80569977]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('hey what is it   talk  what is it an exclusive group of some wp talibanswho are good at destroying selfappointed purist who gang up any one who asks them questions abt their antisocial and destructive noncontribution at wp  ask sityush to clean up his behavior than issue me nonsensical warnings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__.toxic',), array([0.86063683]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('amazoncom is not a reputable source neither is the dust cover of his book again youre trying to turn this into an informercial for this relatively unknown individuals controversial theories maybe hes a genius maybe hes an idiot but it isnt accepted opinion and hence it shouldnt be in an encyclopedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
